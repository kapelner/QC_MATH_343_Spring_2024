---
title: "Practice Assignment 1 MATH 343"
author: "Your Name Here"
output: pdf_document
date: "11:59PM March 3"
---

This practice assignment is coupled to the theory assignment and should be worked on concomitantly.

You should have R and RStudio (latest versions) installed to edit this file. You will write code in places marked "TO-DO" to complete the problems. Most of this will be a pure programming assignment but there are some questions that instead ask you to "write a few sentences" which are not R chunks. This is a W class! 

The tools for solving these problems can be found in the class practice lectures. I prefer you to use the methods I taught you. If you google and find esoteric code you don't understand or if you use chat GPT, this doesn't do you too much good in the long run.

To "hand in" the homework, you should follow the github repo setup instructions on the course homepage. Once you have your own class repo e.g. located in ~, make a /labs directory. Then go back to ~ and clone the class repo. Then copy this file into your repo/labs directory. Edits made there can be committed and pushed. You must push this completed file by the due date to avoid late penalties. 

NOT REQUIRED: After you're done, you have the option to compile this file into a PDF (use the "knit to PDF" button on the submenu above). These final PDF's look pretty as it includes the output of your code. You can push the PDF as well. It will look nice in your portfolio.

This lab requires the following packages. You should make they load now:

```{r}
pacman::p_load(ggplot2, rstan, survival, optimx)
```

You should also make sure stan works by running the following code:

```{r}
example(stan_model, package = "rstan", run.dontrun = TRUE)
```


## Problem 2: Metropolis-within-Gibbs Sampling and Stan

Problem 2 in the theory homework assumes a hurdle-extnegbinomial model for count data. As usual, we will generate the real data below from this DGP (which you don't have the luxury of seeing). We will also visualize its histogram:

```{r}
set.seed(1)
n = 50

true_theta_1 = 0.2345
true_theta_2 = 4.5678
true_theta_3 = 0.3456

x = array(NA, n)
for (i in 1 : n){
  x[i] =  if (runif(1) <= true_theta_1){
            0
          } else {
            rnbinom(1, true_theta_2, true_theta_3) + 1
          }
}

ggplot(data.frame(x = x)) + 
  geom_histogram(aes(x = x))
```

Imagine you are looking at this dataset of counts for the first time (i.e. forget that we generated the data ourselves). Why do you think a hurdle model with an extended negative binomial is appropriate?

#TO-DO

We will now implement the Metropolis-within-Gibbs sampler given what you derived in the theoretical assignment. You found that I've provided some boilerplate code from the class demos below. It is your job to do the sampling.

```{r}
num_tot_samples = 1e4
theta_1s = array(NA, num_tot_samples)
theta_2s = array(NA, num_tot_samples)
theta_3s = array(NA, num_tot_samples)

###initialize thetas to be null values
theta_1s[1] = .Machine$double.eps
theta_2s[1] = .Machine$double.eps
theta_3s[1] = .Machine$double.eps


##useful data to cache here
n_0 = sum(x == 0)
n_plus = n - n+0
x_gr_one_minus_one = x[x >= 1] - 1

#possibly initialize phi for Metropolis-Hastings step
phi = #
#save diagnoistics on whether the proposal distribution worked
accept_theta2s = array(TRUE, num_tot_samples)
  
#create log density of kernel of the conditional distribution of theta_2
ln_kernel_theta_2_given_everything_else = function(theta2, theta3){
  #TO-DO
}


###do the sampling
for (s in 2 : num_tot_samples){
  theta_1s[s] = #do gibbs sample here

	#sample beta_0 first
	theta2star = #do metropolis proposal sample here
	#calc r, the Metropolis Ratio
	ln_r = ln_kernel_theta_2_given_everything_else(theta2star, theta_3s[s - 1]) - 
	       ln_kernel_theta_2_given_everything_else(theta_2s[s - 1], theta_3s[s - 1])
	if (is.nan(ln_r) || (runif(1) > exp(ln_r))){
		#reject - set it equal to previous value
		theta2star = theta_2s[s - 1]
		accept_theta2s[t] = FALSE
	} #o/t accept
	theta2s[s] = theta2star
    
  theta_3s[s] = #do gibbs sample here
}
#cleanup
rm(s, theta2star, ln_r, ln_kernel_theta_2_given_everything_else)
```

We diagnose whether or not the proposal distribution worked appropriately by seeing what proportion of proposal values were accepted:

```{r}
mean(accept_theta2s)
```

Comment on whether or not the proposal distribution worked.

#TO-DO

Now we aggregate all three chains together for convenience:

```{r}
gibbs_chain = data.frame(
  theta_1 = theta1s, 
  theta_2 = theta2s, 
  theta_3 = theta3s,
  t = 1 : num_tot_samples
)
rm(theta1s, theta2s, theta3s, accept_theta2s, num_tot_samples)
```


We now assess convergence using plots. Feel free to play around with the `max_t_for_plotting` to get a better visual on the beginning of the chains.

```{r}
max_t_for_plotting = 500
ggplot(gibbs_chain) +
  geom_point(aes(x = t, y = theta_1)) + 
  xlim(0, max_t_for_plotting)
ggplot(gibbs_chain) +
  geom_point(aes(x = t, y = theta_2)) + 
  xlim(0, max_t_for_plotting)
ggplot(gibbs_chain) +
  geom_point(aes(x = t, y = theta_3)) + 
  xlim(0, max_t_for_plotting)
#cleanup
rm(max_t_for_plotting)
```

Where do we burn?

```{r}
t_burn_in = #TO-DO
```

Now we burn:

```{r}
gibbs_chain = #TO-DO
```

Now we assess autocorrelation. Play with the `ell_max` and `r_max` to get the best assessment possible:

```{r}
par(mfrow = c(3, 1))
ell_max = 500
r_max = 1
acf(gibbs_chain$theta_1, 
    xlim = c(0, ell_max + 10), ylim = c(0, r_max), lag.max = ell_max)
acf(gibbs_chain$theta_2, 
    xlim = c(0, ell_max + 10), ylim = c(0, r_max), lag.max = ell_max)
acf(gibbs_chain$theta_3, 
    xlim = c(0, ell_max + 10), ylim = c(0, r_max), lag.max = ell_max)
#cleanup
rm(ell_max, r_max)
```

Where do we thin?

```{r}
t_thin = #TO-DO
```

Now we thin:

```{r}
gibbs_chain = #TO-DO
```

How many iid samples do we have after burning and thinning?

```{r}
#TO-DO
```

What would we change in the above code so we could've had more iid samples?

#TO-DO

Before we do inference, we first source the convenience function we used in class:

```{r}
visualize_chain_and_compute_estimates_and_cr = function(samples, true_value, alpha = 0.05){
  plot(
    ggplot(data.frame(samples = samples)) +
      geom_histogram(aes(x = samples)) +
      geom_vline(xintercept = mean(samples), col = "blue") + 
      geom_vline(xintercept = true_value, col = "green") + 
      geom_vline(xintercept = quantile(samples, .025), col = "red") + 
      geom_vline(xintercept = quantile(samples, .975), col = "red")
  )
  
  list(
    mmse = mean(samples),
    mmae = median(samples),
    theta = true_value,
    cr_one_minus_alpha_theta = c(
      quantile(samples, alpha / 2), 
      quantile(samples, 1 - alpha / 2)
    )
  )
}
```

Now we do inference on all three parameters:

```{r}
visualize_chain_and_compute_estimates_and_cr(gibbs_chain$theta_1, true_theta_1)
```

How did we do on the first parameter?

#TO-DO


```{r}
visualize_chain_and_compute_estimates_and_cr(gibbs_chain$theta_2, true_theta_2)
```

How did we do on the second parameter?

#TO-DO

```{r}
visualize_chain_and_compute_estimates_and_cr(gibbs_chain$theta_3, true_theta_3)
```

How did we do on the third parameter?

#TO-DO

Now we do this exercise with `stan`. We first create the list of data which is passed into stan:

```{r}
stan_model_data = list(
  n_0 = n_0,
  n_plus = n_plus,
  x_gr_one_minus_one = x_gr_one_minus_one
)
```

Now we write the relevant stan code below as a string (no need for a separate .stan file). I've started by specifying the data block. For the parameters block, you'll need your answer from 2(a) on the theoretical homework (which specifies the parameter spaces of all three parameters). For the model block, you'll need the log of the posterior's kernel from 2(c) on the theoretical homework. The log of the gamma function can be called via `lgamma` in stan.

```{r}
stan_model_code = "
  data {
    int<lower=0> n_0;                  //the number of zeroes in the sample
    int<lower=0> n_plus;               //the number of nonzeroes in the sample
    vector[n_plus] x_gr_one_minus_one; //the x's greater than zero minus 1
  }
  
  parameters {
    //TO-DO... make sure you name the parameters theta_1, theta_2, theta_3
  }
  
  model {
    target += //TO-DO
  }
"
```

Now we sample the model using stan:

```{r}
stan_fit = stan(
  seed = 1,
  model_code = stan_model_code,
  model_name = "hurdle_model",
  data = stan_model_data
)
```

Now we do inference on all three parameters:

```{r}
visualize_chain_and_compute_estimates_and_cr(extract(stan_fit)$theta_1, true_theta_1)
```

How did we do on the first parameter?

#TO-DO


```{r}
visualize_chain_and_compute_estimates_and_cr(extract(stan_fit)$theta_2, true_theta_2)
```

How did we do on the second parameter?

#TO-DO

```{r}
visualize_chain_and_compute_estimates_and_cr(extract(stan_fit)$theta_3, true_theta_3)
```

How did we do on the third parameter?

#TO-DO



## Problem 3: The Permutation Test

We will analyze the anorexia dataset in MASS. You can read about it here:

```{r}
rm(list = ls())
?MASS::anorexia
D = MASS::anorexia
```

The data is measured on 72 people before and after treatment. We are interested in the outcome of percentage change in weight. So we first create that variable:

```{r}
D$y = (D$Postwt - D$Prewt) / D$Prewt
```

We will be interested in if there's a different between the two treatment groups: cognitive behavioral treatment (CBT) vs family treatment (FT). So we separate the two datasets now:

```{r}
x1 = D$y[D$Treat == "FT"]
x2 = D$y[D$Treat == "CBT"]
x = c(x1, x2)
n1 = length(x1)
n2 = length(x2)
rm(D)
```

How many possible ways are there to "permute" the dataset into two groups

```{r}
#TO-DO
```

Is it possible to run this many permutations? Yes / no

Instead let's run `B = 100,000`. Pick a test statistic and compute the test statistic for all  

```{r}
B = 1e5
x_ind = 1 : (n1 + n2)
thetahathat_b = array(NA, B)
for (b in 1 : B){
  #TO-DO
  thetahathat_b[b] = #TO-DO
}
```

Now compute the test stat for the real data

```{r}
thetahathat = #TO-DO
```

Declare the alpha value

```{r}
alpha = 0.05
```

Now run the two-sided test for difference in DGP at level alpha

```{r}
#TO-DO
```

What is the conclusion of the test?

#TO-DO

Calculate the p-value of this test.

```{r}
#TO-DO for MA students only
```

## Problem 4: The Bootstrap

Set the number of bootstrap samples to be `B = 100,000`.

```{r}
B = 1e5
```

Use the bootstrap to create a 95% CI for `theta := Med[X_2]` where X_2 is defined as the DGP of the population of weight increases for the CBT group. 

```{r}
thetahathat_b = array(NA, B)
for (b in 1 : B){
  thetahathat_b[b] = #TO-DO
}
#TO-DO
```

Use the bootstrap to test H_a: theta is nonzero where theta is defined as above.

```{r}
#TO-DO
```

Use the bootstrap to create a 95% CI for `theta := Q[X_2, 0.2]` i.e. the 20th percentile where X_2 is defined as before.

```{r}
thetahathat_b = array(NA, B)
for (b in 1 : B){
  thetahathat_b[b] = #TO-DO
}
#TO-DO
```

Use the bootstrap to test H_a: theta is nonzero where theta is defined as above.

```{r}
#TO-DO
```

## Problem 5: Parametric Survival using the Weibull iid DGP

Let's look at the lung dataset's survival by sex. We'll recode their censoring variable to match the definition from class. Group 1 is male and group 2 is female. The values are sorted.

```{r}
rm(list = ls())
is_male = 1 - (survival::lung$sex - 1) #zero is female, one is male
y_1 = survival::lung$time[is_male == 1]
c_1 = 1 - (survival::lung$status[is_male == 1] - 1)
y_2 = survival::lung$time[is_male == 0]
c_2 = 1 - (survival::lung$status[is_male == 0] - 1)
rm(is_male)
```

Using what you derived one the homework, for an iid Weibull DGP with no censoring, write a function that takes in the vector of survival times y and returns the values of the maximum likeliheood estimates of `k` and `lambda`. You'll need to call the `optimx` function within.

```{r}
mle_weibull_iid_compute = function(y){
  k_mle = #TO-DO 
  lambda_mle = #TO-DO
  list(k = k_mle, lambda = lambda_mle)
}
```

Pretend there is no censoring and find the maximum likelihood estimates of `k` and `lambda` for female survival times (group 2 only). 

```{r}
mles = mle_weibull_iid_compute(y_2)
mles$k
mles$lambda
```

Using the maximum likelihood estimates of `k` and `lambda`, compute the maximum likelihood estimate of the mean for female survival times (group 2 only).

```{r}
1 / mles$lambda * gamma(1 + 1 / mles$k)
```


Using what you derived one the homework for the setting where there is censoring at all different times given by the indices where `c_2 = 1`, find the MLE's of `k` and `lambda` for female survival times (group 2 only). You'll need the `optimx` function.

```{r}
#TO-DO
```

Instead of using the optimization package, write an E-M algorithm that can compute the maximum likelihood estimates of `k` and `lambda` for female survival times (group 2 only). Use epsilon = 0.000001. Everything is filled in for you below except the E step and M step. For the M step, use your function you wrote above, `mle_weibull_iid_compute`.

```{r}
EPS = 1e-6
num_maximum_samples = 1e4

#create vectors 
#reasonable starting positions
ks[1] = 1
lambdas[1] = 1 / mean(y_2)

#create copy of y that will store the expected values of the censored responses
y_with_censor_estimates = y
for (t in 2 : num_maximum_samples){
  k = ks[t - 1]
  lambda = lambdas[t - 1]
  
  ###E-step
  for (i in which(c_2 == 1)){
    y_with_censor_estimates[i] = #TO-DO
  }
  
  ###M-step
  #TO-DO
  
  ###check convergence
  if (sqrt(
    (ks[t - 1] - ks[t])^2 +
    (lambdas[t - 1] - lambdas[t])^2
  ) < EPS){
    t_break = t
    break
  }
}

#collect all iterations
em_chain = data.frame(
  ks = ks, 
  lambdas = lambdas,
  t = 1 : num_maximum_samples
)
em_chain[1 : t_break, ]
em_chain[t_break, ]
```

Are the maximum likelihood estimates of `k` and `lambda` around the same as you computed using the optimization function?

#TO-DO

## Problem 6: Nonparametric Survival

Trace out the Kaplan-Meier survival distribution estimate for female survival times (group 2 only).

```{r}
#TO-DO
```

Estimate median survival for females.

```{r}
#TO-DO
```

Run the log rank test to attempt to prove male and female survival are different.

```{r}
#TO-DO
```

