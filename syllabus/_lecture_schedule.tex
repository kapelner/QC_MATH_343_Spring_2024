\begin{enumerate}
\item[Day 1] [35min] the two-proportion z-test as a Wald test, the pooled proportion estimator;  [10min] the approximate two-sample t-test as an approximate two-sample z-test (Wald test); 
[15min] CI for the difference of two proportions

\item[Day 2] [35min] Grid sampling, distribution sampling via kernel grid sampling, disadvantages of grid sampling; [40min] systematic sweep Gibbs sampling, burning the chain, sampling from the semi-conjugate NIG mode

\item[Day 3] [20min] Autocorrelation grid sampling, thinning the chain; [30min] approximate inference with Gibbs samples; [25min] change point detection model  

\item[Day 4] [55min] normal mixture model with data augmentation; [20min] Bayes Factors part I

\item[Day 5] [15min] Bayes Factors part II; [60min] Metropolis algorithm, Metropolis-Hastings algorithm, metropolis-within-Gibbs, transition kernels

\item[Day 6] [10min] equivalence of the two-sided z and $\chi^2$ tests, equivalence of the two-sided t and F tests; shape of the $\chi^2$ and F distributions;  [30min] $\chi^2$ goodness of fit test for multinomial parameters, its $H_0$ and $H_a$, observed and expected counts, the test statistic; [35min] the $\chi^2$ test of independence among two categorical variables, its $H_0$ and $H_a$, observed and expected counts, the test statistic

\item[Day 7] [10min] review of Type I/II errors; [40min] the multiple hypothesis testing / comparison problem, the $2 \times 2$ frequency table of test results, false discoveries; [10min] definition of familywise error rate (FWER), weak FWER control; [10min] Bonferroni procedure 

\item[Day 8] [15min] Sidak procedure; [25min] Simes procedure; [15min] definition of false discovery proportion, false discovery rate (FDR), setting for equivalence of FWER and FDR, statement that the Simes procedure provides strong control of the FDR; [20min] proof that p-values are uniformly distributed under $H_0$

\item[Day 9] \inblue{Midterm I Review}
\item[Day 10] \inred{Midterm I}

%85min
\item[Day 11] [50min] derivation of the score test as a Wald test, the score test statistic for the logistic distribution with known scale parameter; [15min] definition of likelihood ratio (LR) test statistic, statement of its asymptotic convergence to a $\chi^2_1$, LR test (LRT) derivation of the LR statistic for the iid Bernoulli DGP; [10min] Proof of the asymptotic convergence of the LR test statistic part I

%65min
\item[Day 12] [15min] Proof of the asymptotic convergence of the LR test statistic part II;  [50min] generalized likelihood ratio test for reduced models nested in a full model, statement of asymptotic convergence to a $\chi^2$ with degrees of freedom equal to the difference of number of parameters, demonstration that it differs from the goodness of fit test, demonstration in the normal iid DGP; [10min] visualizing the Wald, score and LR tests

%85min
\item[Day 13] [5min] testing entire DGPs; [10min] definition and illustration of the empirical CDF; [20min] one-sample Kolmogorov-Smirnov (KS) test, statement of Kolmogorov distribution and its critical values; [10min] tests of DGP equivalence among two populations; [15min] two-sample KS test; [15min] intro to the the two-sample permutation test



%65min
\item[Day 14] [45min] the two-sample permutation test partitioning the master population, dsicussion of possible test statistics, computational construction of its RET via resampling; [30min] the nonparametric bootstrap procedure, typical use cases, approximate CI construction, approximate hypothesis testing

\item[Day 15] [30min] Identifiability [30min] statistical sufficiency; [15min] statistical ancillarity

\item[Day 16] [34min] exponential families: sufficiency and ancillarity terms; [20min] Normal posterior under laplace prior = Lasso; [20min] Normal posterior under normal prior = Ridge

%80min
\item[Day 17] [20min] causal inference vs statistical inference, counfounding variable, an illustration of one scenario with confounding and one scenario without confounding; [15min] definition of a two-arm treatment vs control experiment, definition of assignment / allocation / manipulation; [10min] the Rubin causal model, counterfactuals, additive treatment effect, selection bias inducing bias in the naive treatment estimator; [30min] response model linear in confounder and noise, noise as an approximate normal realization, the explicit bias term and its explanation;

%70min
\item[Day 18] [30min] randomized experiments, bernoulli trial design, completely randomized trial design, statment that causal estimates are unbiased over errors and randomized assignments, statement that bias is small in any random assignment; [45min] the randomization test vs. the permutation test, hypothesis testing and confidence intervals

\item[Day 19] \inblue{Midterm II Review}
\item[Day 20] \inred{Midterm II}

\item[Day 21] [30min] optimal experimental design; [30min] restricted randomization strategies such as rerandomization, matching; [15min] sequential experimental design

\item[Day 22] [35min] Weibull modeling of survival; [35min] Kaplan-Meier estimate

\item[Day 23] [30min] Proportional Cox model; [30min] Poisson Regression]; [15min] Zero-Inflated Poisson regression

\item[Day 24] [30min] Negative Binomial regression; [20min] Beta regression; [15min] Review of multivariate normal rv

%70min
\item[Day 25] [55min] Derivation of T-test and F-test and chi-squared test linear regression estimator; [20min] derivation of variance-covariance matrix of the least squared estimators

%75min
\item[Day 26] [30min] Introduction to neural networks; [45min] estimation of parameters in neural networks using optimization algorithms

%80min
\item[Day 27] [30min] Introduction to deepl learning; [45min] convolutional neural networks for image modeling

\item[Day 28] \inblue{Final Review}

\end{enumerate}

